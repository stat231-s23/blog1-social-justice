---
title: "Text Analysis of The Amherst Student Articles"
author: "Anna Zhou and Julia Woodward"
date: "\\today"
output: pdf_document
---

```{r}
#loading packages

library(tidytext)
library(tidyverse)
library(wordcloud)
library(textdata)
library(ggplot2)
library(scales)
```

```{r}
#read in data
opinions <- read_csv("data/opinions.csv")
```

## **Categorizing Titles**

```{r}
#categorizing article titles by topic 
Title = opinions %>% select("Title")
all_words_in_titles <- Title %>%
  unnest_tokens(output = word, input = Title)

policy <- all_words_in_titles %>% filter(word == "AAS" | word == "requirements" | word == "attendance"| word == "curriculum"| word == "constitution"| word == "j-term"| word == "mask"| word == "class"| word == "covid" | word == "space" | word == "policies" | word == "haven")

finances <- all_words_in_titles %>% filter(word == "aid" | word == "tuition"| word == "pay"| word == "money" | word == "fee" | word == "financial")

health <- all_words_in_titles %>% filter(word == "illness" | word == "health"| word == "ableism" | word == "cancer" | word == "pandemic" | word == "mask" | word == "bruce")

politics <- all_words_in_titles %>% filter(word == "macro-" | word == "tragedy" | word == "columbus" | word == "israel" | word == "insider" | word == "discourse")

sports <- all_words_in_titles %>% filter(word == "bruce" | word == "badminton")

```

```{r}
#creating dataframe with number of articles per topic

topics <- c("policy", "health", "finances", "politics", "sports")
count <- c(as.numeric(count(policy)), as.numeric(count(health)), as.numeric(count(finances)), as.numeric(count(politics)), as.numeric(count(sports)))
title_topics <- data.frame(topics, count)
title_topics

ggplot(data = title_topics, aes(x= topics, y = count, color = topics, fill = topics)) + 
  geom_col() + 
  labs(
    x = "Topics",
    y = "Number of articles",
    title = "Categorizing The Student Article Titles by Topic")
```

## Term Frequency

```{r}
#finding most common terms in the body section
Body = opinions %>% select("Body")

all_words_in_body <- Body %>%
  unnest_tokens(output = word, input = Body)

data(stop_words)

stop_words %>% 
  count(lexicon)

head(stop_words)
body_words <- all_words_in_body %>% 
  filter(word != "student"  & word!= "it’s") %>%
  anti_join(stop_words, by="word")

body_words %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  ggplot(aes(x = fct_reorder(word, n), y = n, color = word, fill = word)) +
  geom_col() +
  # Rotate graph
  coord_flip() +
  guides(color = "none", 
         fill = "none") +
  labs(
    x = NULL,
    y = "Number of instances",
    title = "The most common words in Amherst Student Opinion Articles")
```

```{r}
#finding most common bigrams (2-term phrases)
#creating bigram token 
opinions_bigrams <- Body %>%
  unnest_tokens(output = bigram, input = Body, token = "ngrams", n = 2)

opinions_bigrams_clean <- opinions_bigrams %>%
  separate(col="bigram", into=c("word1", "word2"), sep=" ", remove=FALSE) %>%
  anti_join(stop_words, by=c("word1"="word")) %>%
  anti_join(stop_words, by=c("word2"="word")) %>%
  filter(!is.na(bigram)) %>%
  #removing irrelevant/uninformative bigrams
  filter((!grepl("amherst|editorial board|email protected|articles comment|emailing email", bigram))) %>%
  count(bigram, sort = TRUE) %>%
  slice(1:10)

#plotting most common bigrams
ggplot(opinions_bigrams_clean, aes(x = fct_reorder(bigram, n), y = n
                                , color = bigram, fill = bigram)) +
geom_col() +
# Rotate graph 
  coord_flip() + guides(color = "none",
         fill = "none") +
  labs(
x = NULL,
    y = "Number of instances",
    title = "Most common bigrams in Amherst Student Opinion articles")

```


```{r}
#creating word cloud

opinion_word_freqs <- opinions %>%
  unnest_tokens(output = word, input = Body) %>%
  anti_join(stop_words, by = "word") %>%
  #filtering out irrelevant words
  filter(word != "student" & word != "it’s" & word != "i’m" & word != "don’t" & word != "i’ve") %>%
  count(word, sort = TRUE)

#setting seed for reproducibility
set.seed(100)

#choosing color palette
my_palette <- brewer.pal(10, "Dark2")

opinion_word_freqs %>%
  with(wordcloud(words = word,
      freq = n,
      min.freq = 20,
      max.words = 50,
    # Plot the words in a random order
      random.order = TRUE,
    # Specify the range of the size of the words
      scale = c(2, 0.3),
# Specify proportion of words with 90 degree rotation 
      rot.per = 0.15,
# Color words from least to most frequent
      colors = my_palette,
# Change font family
      family = "sans"))
```

## Sentiment Analysis

#### AFINN Lexicon

```{r}
afinn_lexicon <- get_sentiments("afinn")

#creating dataset using afinn lexicon grouped by date
opinion_afinn <- opinions %>%
  unnest_tokens(word, Body) %>%
  anti_join(get_stopwords(), by = "word") %>%
  inner_join(get_sentiments("afinn"))

#getting word count for each sentiment value (-5 to 5)
opinion_afinn_count <- opinion_afinn %>%
  count(value)

#displaying table
kable(opinion_afinn_count)
```


```{r}
#displaying barplot
ggplot(opinion_afinn_count, aes(x = value, y = n, fill = value)) +
  geom_col() +
  labs(
    x = "Sentiment",
    y = "Number of words in Student Opinion Articles",
    title = "Sentiment value according to AFINN Lexicon") 
```


```{r}
#plotting graph of sentiment in articles over time
opinion_narrative <- opinion_afinn %>%
  group_by(Date) %>%
  summarize(sentiment = sum(value))
  
ggplot(opinion_narrative, aes(x = Date, y = sentiment)) + 
  geom_point() + 
  scale_x_date(breaks = date_breaks("months"), labels = date_format("%B")) +
  geom_smooth() +  
  labs(title = "Sentiment in The Student Over Time by Issue Release Date Using AFINN Lexicon", subtitle = ("9/14/2022 - 4/26/2023")) 
```

```{r}
# overall sentiment score
opinion_afinn %>%
  summarize(total_score = sum(value))

# by article
afinn_byarticles <- opinion_afinn %>%
  group_by(Title) %>%
  summarize(article_score = sum(value))
ggplot(data = afinn_byarticles, aes(x = article_score)) +
  geom_density(color="#AF7AC5", fill="#AF7AC5", alpha = 0.5) +
  theme_classic() +
  labs(title = "Total Article Sentiment Score based on AFINN Lexicon", x = "Article Sentiment Score")
```


```{r}
# overall favstats by article -- median of 29 is positive
mosaic::favstats(~article_score, data=afinn_byarticles)
```

```{r}
#AFINN sentiment for articles about Amherst policies

policy_words <- opinions %>%
  filter(str_detect(str_to_lower(Title), "policy|policies|financial aid|curriculum|case|fee|requirements|tuition|ableism")) %>%
  unnest_tokens(output=word, input=Body, token="words") %>%
  anti_join(stop_words, by="word") %>%
  inner_join(afinn_lexicon, by="word") %>%
  group_by(Title) %>%
  mutate(seq=row_number())

ggplot(policy_words, aes(x=seq, y=value)) +
  geom_line() +
  geom_hline(yintercept=0, lty="dashed", color="red") +
  facet_wrap(~Title, labeller = labeller(Title = label_wrap_gen(width = 23))) +
  labs(x="Order", y="AFINN Sentiment")
```

```{r}
# non-stop words not in AFINN
opinions %>%
  filter(str_detect(str_to_lower(Title), "policy|policies|financial aid|curriculum|case|fee|requirements|tuition|ableism")) %>%
  unnest_tokens(output=word, input=Body, token="words") %>%
  anti_join(stop_words, by="word") %>%
  nrow()
```

```{r}
opinions %>%
  filter(str_detect(str_to_lower(Title), "policy|policies|financial aid|curriculum|case|fee|requirements|tuition|ableism")) %>%
  unnest_tokens(output=word, input=Body, token="words") %>%
  anti_join(stop_words, by="word") %>%
  anti_join(afinn_lexicon, by="word") %>%
  nrow()
```

```{r}
4318/4741
```

#### NRC Lexicon

```{r}
nrc_lexicon <- get_sentiments("nrc")

# identify words in word_frequencies dataset (which has stop words removed) that are not in the lexicon
nrc_missed_words <- opinion_word_freqs %>%
  anti_join(nrc_lexicon, by = "word")

# proportion missed
nrow(nrc_missed_words)/nrow(opinion_word_freqs)
```

Beware -- over 79% of the words in the Amherst Student articles are not found in the nrc lexicon.

```{r}
#top words by sentiment
nrc_opinions <- opinion_word_freqs %>%
  inner_join(nrc_lexicon, by = "word") %>%
  filter(sentiment %in% c("positive", "negative", "anger", "disgust"
                          , "joy", "sadness")) %>%
  arrange(sentiment, desc(n)) %>%
  group_by(sentiment) %>%
  slice(1:10)

#plotting top 10 words under each sentiment
ggplot(data=nrc_opinions, aes(x = reorder(word,n), y = n, fill = as.factor(n))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Frequency") +
  facet_wrap(~ sentiment, ncol = 2, scales = "free") +
  coord_flip()
```



```{r}

opinions_nrc <- opinions %>%
  unnest_tokens(word, Body) %>%
  #removing most common irrelevant/incorrectly classified words
  filter(word != "liberal" & word != "rail" & word != "anonymous" & word != "pay" & word != "president" & word != "question" & word != "library") %>%
  anti_join(get_stopwords(), by = "word") %>%
  inner_join(get_sentiments("nrc"), multiple = "all")

#creating dataset with top15 most frequent words classified as negative
nrc_negative <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

opinions_neg <- opinions_nrc %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE) %>%
  slice(1:15)

#creating dataset with top15 most frequent words classified as positive
nrc_positive <- get_sentiments("nrc") %>% 
  filter(sentiment == "positive")

opinions_pos <- opinions_nrc %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE) %>%
  slice(1:15)
```

```{r}
#table of top 15 "negative" words

kable(opinions_neg, col.names = c("Word", "Frequency"), caption = "Top 30 Words in the Student Opinion articles classified as 'negative' in NRC lexicon by frequency") %>%
  kable_styling(font_size = 8) %>%
  row_spec(0,bold=TRUE) 
```

```{r}
#table of top 15 "positive" words

kable(opinions_pos, col.names = c("Word", "Frequency"), caption = "Top 30 Words in the Student Opinion articles classified as 'positive' in NRC lexicon by frequency") %>%
  kable_styling(font_size = 8) %>%
  row_spec(0,bold=TRUE) 
```



